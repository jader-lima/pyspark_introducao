{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba138d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, expr\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType, IntegerType, ArrayType, DateType\n",
    "import sys\n",
    "import os\n",
    "from delta import DeltaTable\n",
    "from delta.tables import *\n",
    "\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import io\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b9eec62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark_packages_list = [\n",
    "            'io.delta:delta-core_2.12:2.4.0',\n",
    "            'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0',\n",
    "            'org.mongodb.spark:mongo-spark-connector:10.0.2'\n",
    "        ]\n",
    "    warehouse_location = '/mnt/datalake/warehouse'\n",
    "    #bin/spark-sql --packages io.delta:delta-core_2.12:2.4.0 --conf \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\" --conf \"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\"\n",
    "\n",
    "    spark_packages = \",\".join(spark_packages_list)\n",
    "#     derby_location = '/mnt/datalake/derby'\n",
    "#     .config(\"spark.driver.extraJavaOptions\", f\"Dderby.system.home='{derby_location}'\") \\\n",
    "    return SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"File Streaming Demo\") \\\n",
    "        .master(\"local[3]\") \\\n",
    "        .config(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "        .config(\"spark.jars.packages\", spark_packages) \\\n",
    "        .config(\"spark.streaming.stopGracefullyOnShutdown\", \"true\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"delta.deletedFileRetentionDuration\",7)\\\n",
    "        .config(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"false\") \\\n",
    "        .config(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1af499-71df-4d41-a28d-ff0da85e530d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c94b6e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/31 17:04:31 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eddd5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/29 21:15:49 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/12/29 21:15:49 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/12/29 21:15:52 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "23/12/29 21:15:52 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore jovyan@172.31.0.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Database(name='default', catalog='spark_catalog', description='Default Hive database', locationUri='file:/mnt/datalake/warehouse')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0334b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/29 21:15:55 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5eb925c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([ \n",
    "    StructField(\"product_id\",IntegerType(),True), \\\n",
    "    StructField(\"product_name\",StringType(),True),\n",
    "    StructField(\"product_category_name\",StringType(),True), \\\n",
    "    StructField(\"product_price\", DoubleType(), True),\n",
    "    StructField(\"status\",StringType(),True),\n",
    "  ])\n",
    "\n",
    "data_product = [(1,'perfurme magico','perfumaria',100.5,'Inative'),\n",
    "                (2,'replica vaso ming importado','artes',70.75,'Active'),\n",
    "                (3,'raquete de tenis nacional','esporte_lazer',365.00,'Active'),\n",
    "                (4,'mordedor','bebes',27.25,'Inative'),\n",
    "                (5,'televisor 46 polegadas 4k','utilidades_domesticas',2555.55,'Active'),\n",
    "                (6,'PS','utilidades_domesticas',2555.55,'Active')\n",
    "               ]\n",
    "\n",
    "\n",
    "df_products = spark.createDataFrame(data=data_product,schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "99ddd3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------------------+-------------+-------+\n",
      "|product_id|        product_name|product_category_name|product_price| status|\n",
      "+----------+--------------------+---------------------+-------------+-------+\n",
      "|         1|     perfurme magico|           perfumaria|        100.5|Inative|\n",
      "|         2|replica vaso ming...|                artes|        70.75| Active|\n",
      "|         3|raquete de tenis ...|        esporte_lazer|        365.0| Active|\n",
      "|         4|            mordedor|                bebes|        27.25|Inative|\n",
      "|         5|televisor 46 pole...| utilidades_domest...|      2555.55| Active|\n",
      "|         6|                  PS| utilidades_domest...|      2555.55| Active|\n",
      "+----------+--------------------+---------------------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc0b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "location_tb  = '/mnt/datalake/bronze/history/products/'\n",
    "merge_condition = \"tgt.product_id = src.product_id\"\n",
    "\n",
    "columns = [\n",
    "    StructField('product_id', IntegerType(), True),\n",
    "    StructField('product_name', StringType(), True),\n",
    "    StructField('product_category_name', StringType(), True),\n",
    "    StructField('product_price', DoubleType(), True),\n",
    "    StructField('status', StringType(), True)\n",
    "]\n",
    "\n",
    "if (DeltaTable.isDeltaTable(spark, location_tb)):\n",
    "    print('1')\n",
    "    deltaTable = DeltaTable.forPath(spark, location_tb)\n",
    "    deltaTable.alias('tgt') \\\n",
    "        .merge(\n",
    "            df_products.alias('src'),\n",
    "            merge_condition\n",
    "        ) \\\n",
    "        .whenMatchedUpdateAll() \\\n",
    "        .whenNotMatchedInsertAll() \\\n",
    "        .execute()\n",
    "else:\n",
    "    print('2')\n",
    "    print(DeltaTable.isDeltaTable(spark, location_tb))\n",
    "    DeltaTable \\\n",
    "            .create(spark) \\\n",
    "            .addColumns(columns) \\\n",
    "            .location(location_tb) \\\n",
    "            .execute()\n",
    "\n",
    "    deltaTable = DeltaTable.forPath(spark, location_tb)\n",
    "    deltaTable.alias('tgt') \\\n",
    "        .merge(\n",
    "            df_products.alias('src'),\n",
    "            merge_condition\n",
    "        ) \\\n",
    "        .whenMatchedUpdateAll() \\\n",
    "        .whenNotMatchedInsertAll() \\\n",
    "        .execute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f57cb0",
   "metadata": {},
   "source": [
    "### History em Delta Tables <br>\n",
    "atravez do método **DeltaTable.forPath(spark, caminho_Delta_table)**\n",
    "\n",
    "**Importante** Uma DeltaTable não é um spark dataframe, então caso seja necessário utilizar operações e ações de um <br> \n",
    "spark dataframe existem duas formas :\n",
    "* Carregar os dados gravados no formato delta para um dataframe pyspark,utilizar o spark.read.format('delta').load(caminho_Delta_table) \n",
    "\n",
    "* Carregar uma DeltaTable com **DeltaTable.forPath(spark, caminho_Delta_table)** e usar o metodo **toDf()** , que retorna um dataframe spark de uma tabela delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2f57c07a-904c-44f1-8dd1-4ecacaff098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta import DeltaTable\n",
    "from delta.tables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "89e1a88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|   operation| operationParameters| job|notebook|clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|     11|2023-12-31 16:55:...|  null|    null|  VACUUM END|{status -> COMPLE...|null|    null|     null|         10|SnapshotIsolation|         true|{numDeletedFiles ...|        null|Apache-Spark/3.4....|\n",
      "|     10|2023-12-31 16:55:...|  null|    null|VACUUM START|{retentionCheckEn...|null|    null|     null|          9|SnapshotIsolation|         true|{numFilesToDelete...|        null|Apache-Spark/3.4....|\n",
      "|      9|2023-12-31 16:52:...|  null|    null|  VACUUM END|{status -> COMPLE...|null|    null|     null|          8|SnapshotIsolation|         true|{numDeletedFiles ...|        null|Apache-Spark/3.4....|\n",
      "|      8|2023-12-31 16:52:...|  null|    null|VACUUM START|{retentionCheckEn...|null|    null|     null|          7|SnapshotIsolation|         true|{numFilesToDelete...|        null|Apache-Spark/3.4....|\n",
      "|      7|2023-12-31 16:46:...|  null|    null|  VACUUM END|{status -> COMPLE...|null|    null|     null|          6|SnapshotIsolation|         true|{numDeletedFiles ...|        null|Apache-Spark/3.4....|\n",
      "|      6|2023-12-31 16:46:...|  null|    null|VACUUM START|{retentionCheckEn...|null|    null|     null|          5|SnapshotIsolation|         true|{numFilesToDelete...|        null|Apache-Spark/3.4....|\n",
      "|      5|2023-12-31 16:14:...|  null|    null|     RESTORE|{version -> 1, ti...|null|    null|     null|          4|     Serializable|        false|{numRestoredFiles...|        null|Apache-Spark/3.4....|\n",
      "|      4|2023-12-31 16:12:...|  null|    null|     RESTORE|{version -> 2, ti...|null|    null|     null|          3|     Serializable|        false|{numRestoredFiles...|        null|Apache-Spark/3.4....|\n",
      "|      3|2023-12-31 16:10:...|  null|    null|     RESTORE|{version -> 0, ti...|null|    null|     null|          2|     Serializable|        false|{numRestoredFiles...|        null|Apache-Spark/3.4....|\n",
      "|      2|2023-12-29 21:42:...|  null|    null|     RESTORE|{version -> 0, ti...|null|    null|     null|          1|     Serializable|        false|{numRestoredFiles...|        null|Apache-Spark/3.4....|\n",
      "|      1|2023-12-29 14:25:...|  null|    null|       MERGE|{predicate -> (tg...|null|    null|     null|          0|     Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.2....|\n",
      "|      0|2023-12-29 14:25:...|  null|    null|CREATE TABLE|{isManaged -> fal...|null|    null|     null|       null|SnapshotIsolation|         true|                  {}|        null|Apache-Spark/3.2....|\n",
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_products = '/mnt/datalake/bronze/history/products/'\n",
    "deltaTable = DeltaTable.forPath(spark, path_products)\n",
    "fullHistoryDF = deltaTable.history()\n",
    "fullHistoryDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2db3514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------------+---------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|version|timestamp              |operation   |operationParameters                                                                                                                                |\n",
      "+-------+-----------------------+------------+---------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1      |2023-12-29 14:25:45.397|MERGE       |{predicate -> (tgt.product_id = src.product_id), matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}]}|\n",
      "|0      |2023-12-29 14:25:41.145|CREATE TABLE|{isManaged -> false, description -> null, partitionBy -> [], properties -> {}}                                                                     |\n",
      "+-------+-----------------------+------------+---------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fullHistoryDF.select(\"version\", \"timestamp\", \"operation\", \"operationParameters\").show(10, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10965e49",
   "metadata": {},
   "source": [
    "### Descrição dos campos retornados pelo history ###\n",
    "\n",
    "Column|Type|Description\n",
    "------|----|-----------\n",
    "version|\tlong|\tTable version generated by the operation.\n",
    "timestamp|\ttimestamp|\tWhen this version was committed.\n",
    "userId|\tstring|\tID of the user that ran the operation.\n",
    "userName|\tstring|\tName of the user that ran the operation.\n",
    "operation|\tstring|\tName of the operation.\n",
    "operationParameters|\tmap|\tParameters of the operation (for example, predicates.)\n",
    "job|\tstruct|\tDetails of the job that ran the operation.\n",
    "notebook|\tstruct|\tDetails of notebook from which the operation was run.\n",
    "clusterId|\tstring|\tID of the cluster on which the operation ran.\n",
    "readVersion|\tlong|\tVersion of the table that was read to perform the write operation.\n",
    "isolationLevel|\tstring|\tIsolation level used for this operation.\n",
    "isBlindAppend|\tboolean|\tWhether this operation appended data.\n",
    "operationMetrics|\tmap|\tMetrics of the operation (for example, number of rows and files modified.)\n",
    "userMetadata|\tstring|\tUser-defined commit metadata if it was specified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fe4a9d-bfe0-4826-a3aa-a75175c04c66",
   "metadata": {},
   "source": [
    "### Versioning em Delta Tables <br>\n",
    "É possivel carregar um dataframe spark com uma versão especifica existente, somente adicionando a option <br>\n",
    "**option(\"versionAsOf\", numero_da_versao)** ,  no exemplo, basta substituir o numero_da_versão por qualquer número de versão existente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c4dcd2-ae39-49fa-83c1-95ed7317c26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_products = '/mnt/datalake/bronze/history/products/'\n",
    "df_products_v0 = spark.read.format('delta').option(\"versionAsOf\", 0).load(path_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a303e960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---------------------+-------------+------+\n",
      "|product_id|product_name|product_category_name|product_price|status|\n",
      "+----------+------------+---------------------+-------------+------+\n",
      "+----------+------------+---------------------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_products_v0.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9f2352c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_products = '/mnt/datalake/bronze/history/products/'\n",
    "df_products_v1 = spark.read.format('delta').option(\"versionAsOf\", 1).load(path_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ad05663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------------------+-------------+-------+\n",
      "|product_id|        product_name|product_category_name|product_price| status|\n",
      "+----------+--------------------+---------------------+-------------+-------+\n",
      "|         1|     perfurme magico|           perfumaria|        100.5|Inative|\n",
      "|         2|replica vaso ming...|                artes|        70.75| Active|\n",
      "|         3|raquete de tenis ...|        esporte_lazer|        365.0| Active|\n",
      "|         4|            mordedor|                bebes|        27.25|Inative|\n",
      "|         6|                  PS| utilidades_domest...|      2555.55| Active|\n",
      "|         5|televisor 46 pole...| utilidades_domest...|      2555.55| Active|\n",
      "+----------+--------------------+---------------------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_products_v1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4219588e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------------------+-------------+-------+\n",
      "|product_id|        product_name|product_category_name|product_price| status|\n",
      "+----------+--------------------+---------------------+-------------+-------+\n",
      "|         1|     perfurme magico|           perfumaria|        100.5|Inative|\n",
      "|         2|replica vaso ming...|                artes|        70.75| Active|\n",
      "|         3|raquete de tenis ...|        esporte_lazer|        365.0| Active|\n",
      "|         4|            mordedor|                bebes|        27.25|Inative|\n",
      "|         6|                  PS| utilidades_domest...|      2555.55| Active|\n",
      "|         5|televisor 46 pole...| utilidades_domest...|      2555.55| Active|\n",
      "+----------+--------------------+---------------------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").option(\"versionAsOf\", \"1\").load(path_products).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf081f04-a663-4271-b23c-725005309d3f",
   "metadata": {},
   "source": [
    "### Restore de versions em Delta Tables <br>\n",
    "É possivel carregar um dataframe spark com uma versão especifica existente, somente adicionando a option <br>\n",
    "**option(\"versionAsOf\", numero_da_versao)** ,  no exemplo, basta substituir o numero_da_versão por qualquer número de versão existente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fbbe24be",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_products = '/mnt/datalake/bronze/history/products/'\n",
    "deltaTable = DeltaTable.forPath(spark, path_products)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2207b175-040c-4ee6-b60d-44d6aa1e048b",
   "metadata": {},
   "source": [
    "No exemplo abaixo, utilizamos a versão 0, onde somente criamos a delta table sem dados, o comando show retorna somente o cabeçalho dos dados como esperado <br>\n",
    "**deltaTable.restoreToVersion(0)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ef79a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[table_size_after_restore: bigint, num_of_files_after_restore: bigint, num_removed_files: bigint, num_restored_files: bigint, removed_files_size: bigint, restored_files_size: bigint]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaTable.restoreToVersion(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01c1e6ca-50e1-4070-954d-5f076031a0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---------------------+-------------+------+\n",
      "|product_id|product_name|product_category_name|product_price|status|\n",
      "+----------+------------+---------------------+-------------+------+\n",
      "+----------+------------+---------------------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_delta = deltaTable.toDF()\n",
    "df_from_delta.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d8199b-e1c3-4605-a363-1fa49033c655",
   "metadata": {},
   "source": [
    "Para demonstrar o a mudança dos dados, escolhemos a versão 1, onde o comando de **MERGE** foi realizado,  com isso uma nova <br> versão é criada com os dados da versão 1 , a versão criada é sempre mais atual. <br>\n",
    " **deltaTable.restoreToVersion(1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad5abc27-2e41-4cf2-bcf4-48a980358be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[table_size_after_restore: bigint, num_of_files_after_restore: bigint, num_removed_files: bigint, num_restored_files: bigint, removed_files_size: bigint, restored_files_size: bigint]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaTable.restoreToVersion(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7ec717b9-f4ff-4ca2-b381-cd93ee3e2c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|   operation| operationParameters| job|notebook|clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|      5|2023-12-31 16:14:...|  null|    null|     RESTORE|{version -> 1, ti...|null|    null|     null|          4|     Serializable|        false|{numRestoredFiles...|        null|Apache-Spark/3.4....|\n",
      "|      4|2023-12-31 16:12:...|  null|    null|     RESTORE|{version -> 2, ti...|null|    null|     null|          3|     Serializable|        false|{numRestoredFiles...|        null|Apache-Spark/3.4....|\n",
      "|      3|2023-12-31 16:10:...|  null|    null|     RESTORE|{version -> 0, ti...|null|    null|     null|          2|     Serializable|        false|{numRestoredFiles...|        null|Apache-Spark/3.4....|\n",
      "|      2|2023-12-29 21:42:...|  null|    null|     RESTORE|{version -> 0, ti...|null|    null|     null|          1|     Serializable|        false|{numRestoredFiles...|        null|Apache-Spark/3.4....|\n",
      "|      1|2023-12-29 14:25:...|  null|    null|       MERGE|{predicate -> (tg...|null|    null|     null|          0|     Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.2....|\n",
      "|      0|2023-12-29 14:25:...|  null|    null|CREATE TABLE|{isManaged -> fal...|null|    null|     null|       null|SnapshotIsolation|         true|                  {}|        null|Apache-Spark/3.2....|\n",
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fullHistoryDF = deltaTable.history()\n",
    "fullHistoryDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "14a4c179-ef1e-4766-85a5-2b8373f858c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------------------+-------------+-------+\n",
      "|product_id|        product_name|product_category_name|product_price| status|\n",
      "+----------+--------------------+---------------------+-------------+-------+\n",
      "|         1|     perfurme magico|           perfumaria|        100.5|Inative|\n",
      "|         2|replica vaso ming...|                artes|        70.75| Active|\n",
      "|         3|raquete de tenis ...|        esporte_lazer|        365.0| Active|\n",
      "|         4|            mordedor|                bebes|        27.25|Inative|\n",
      "|         6|                  PS| utilidades_domest...|      2555.55| Active|\n",
      "|         5|televisor 46 pole...| utilidades_domest...|      2555.55| Active|\n",
      "+----------+--------------------+---------------------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_delta = deltaTable.toDF()\n",
    "df_from_delta.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929036f4-deaa-4adb-a8ee-a84bbe231065",
   "metadata": {},
   "source": [
    "### Vaccum em Delta Tables <br>\n",
    "É possivel remover dados antigos de uma Delta Table utilizando o comando **Vacuum** . <br>\n",
    "Essa opção se torna interessante por uma Delta Table vai acumular diversas versão criando maior custo de armazenagem de dados <br>\n",
    "Após rodar o comando **vacuum** , os dados serão permanentemente removidos da Delta Table, o comando **vacuum** deve ser usado com cuidado <br>\n",
    "e combinado com uma estrátégia de retenção de dados. <br>\n",
    "No Exemplo abaixo o comando **vacuum** é disparado com o periodo default de retenção de dados. <br>\n",
    "No segundo exemplo, o comando **vacuum** é disparado removendo versão com mais de 10 horas.Como o periodo de retenção é pequeno,<br>\n",
    "o spark não vai executar o comando e vai informar que é necessário acrescentar a configuração\n",
    "**spark.databricks.delta.retentionDurationCheck.enabled = false** no Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "56bb0e2b-c77e-4f57-8438-12efa72608e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|   operation| operationParameters| job|notebook|clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|      5|2023-12-31 16:14:...|  null|    null|     RESTORE|{version -> 1, ti...|null|    null|     null|          4|     Serializable|        false|{numRestoredFiles...|        null|Apache-Spark/3.4....|\n",
      "|      4|2023-12-31 16:12:...|  null|    null|     RESTORE|{version -> 2, ti...|null|    null|     null|          3|     Serializable|        false|{numRestoredFiles...|        null|Apache-Spark/3.4....|\n",
      "|      3|2023-12-31 16:10:...|  null|    null|     RESTORE|{version -> 0, ti...|null|    null|     null|          2|     Serializable|        false|{numRestoredFiles...|        null|Apache-Spark/3.4....|\n",
      "|      2|2023-12-29 21:42:...|  null|    null|     RESTORE|{version -> 0, ti...|null|    null|     null|          1|     Serializable|        false|{numRestoredFiles...|        null|Apache-Spark/3.4....|\n",
      "|      1|2023-12-29 14:25:...|  null|    null|       MERGE|{predicate -> (tg...|null|    null|     null|          0|     Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.2....|\n",
      "|      0|2023-12-29 14:25:...|  null|    null|CREATE TABLE|{isManaged -> fal...|null|    null|     null|       null|SnapshotIsolation|         true|                  {}|        null|Apache-Spark/3.2....|\n",
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fullHistoryDF = deltaTable.history()\n",
    "fullHistoryDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0b45ecb9-8b0c-4f14-aa29-1d95fe5f4bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:/mnt/datalake/warehouse\n"
     ]
    }
   ],
   "source": [
    "conf = spark.sparkContext.getConf().get('spark.sql.warehouse.dir')\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "132f54c5-3b47-4ed0-889c-9eba03698f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 365:=====================================================> (49 + 1) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 0 files and directories in a total of 1 directories.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaTable.vacuum()        # vacuum files not required by versions older than the default retention period\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c0718496-90c5-4618-a228-b87e598af285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|   operation| operationParameters| job|notebook|clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|      7|2023-12-31 16:46:...|  null|    null|  VACUUM END|{status -> COMPLE...|null|    null|     null|          6|SnapshotIsolation|         true|{numDeletedFiles ...|        null|Apache-Spark/3.4....|\n",
      "|      6|2023-12-31 16:46:...|  null|    null|VACUUM START|{retentionCheckEn...|null|    null|     null|          5|SnapshotIsolation|         true|{numFilesToDelete...|        null|Apache-Spark/3.4....|\n",
      "|      5|2023-12-31 16:14:...|  null|    null|     RESTORE|{version -> 1, ti...|null|    null|     null|          4|     Serializable|        false|{numRestoredFiles...|        null|Apache-Spark/3.4....|\n",
      "|      4|2023-12-31 16:12:...|  null|    null|     RESTORE|{version -> 2, ti...|null|    null|     null|          3|     Serializable|        false|{numRestoredFiles...|        null|Apache-Spark/3.4....|\n",
      "|      3|2023-12-31 16:10:...|  null|    null|     RESTORE|{version -> 0, ti...|null|    null|     null|          2|     Serializable|        false|{numRestoredFiles...|        null|Apache-Spark/3.4....|\n",
      "|      2|2023-12-29 21:42:...|  null|    null|     RESTORE|{version -> 0, ti...|null|    null|     null|          1|     Serializable|        false|{numRestoredFiles...|        null|Apache-Spark/3.4....|\n",
      "|      1|2023-12-29 14:25:...|  null|    null|       MERGE|{predicate -> (tg...|null|    null|     null|          0|     Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.2....|\n",
      "|      0|2023-12-29 14:25:...|  null|    null|CREATE TABLE|{isManaged -> fal...|null|    null|     null|       null|SnapshotIsolation|         true|                  {}|        null|Apache-Spark/3.2....|\n",
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fullHistoryDF = deltaTable.history()\n",
    "fullHistoryDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "46509af7-dcb4-454e-8ed6-419ee775a0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/31 16:51:52 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cd20725b-a919-44bb-80c3-21d6deb482ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 629:============================================>          (40 + 3) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 0 files and directories in a total of 1 directories.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaTable.vacuum(5)   # vacuum files not required by versions more than 5 hours old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "155c1564-88c9-4133-89b0-ca0510f8d46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|   operation| operationParameters| job|notebook|clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|     13|2023-12-31 17:05:...|  null|    null|  VACUUM END|{status -> COMPLE...|null|    null|     null|         12|SnapshotIsolation|         true|{numDeletedFiles ...|        null|Apache-Spark/3.4....|\n",
      "|     12|2023-12-31 17:05:...|  null|    null|VACUUM START|{retentionCheckEn...|null|    null|     null|         11|SnapshotIsolation|         true|{numFilesToDelete...|        null|Apache-Spark/3.4....|\n",
      "|     11|2023-12-31 16:55:...|  null|    null|  VACUUM END|{status -> COMPLE...|null|    null|     null|         10|SnapshotIsolation|         true|{numDeletedFiles ...|        null|Apache-Spark/3.4....|\n",
      "|     10|2023-12-31 16:55:...|  null|    null|VACUUM START|{retentionCheckEn...|null|    null|     null|          9|SnapshotIsolation|         true|{numFilesToDelete...|        null|Apache-Spark/3.4....|\n",
      "|      9|2023-12-31 16:52:...|  null|    null|  VACUUM END|{status -> COMPLE...|null|    null|     null|          8|SnapshotIsolation|         true|{numDeletedFiles ...|        null|Apache-Spark/3.4....|\n",
      "|      8|2023-12-31 16:52:...|  null|    null|VACUUM START|{retentionCheckEn...|null|    null|     null|          7|SnapshotIsolation|         true|{numFilesToDelete...|        null|Apache-Spark/3.4....|\n",
      "|      7|2023-12-31 16:46:...|  null|    null|  VACUUM END|{status -> COMPLE...|null|    null|     null|          6|SnapshotIsolation|         true|{numDeletedFiles ...|        null|Apache-Spark/3.4....|\n",
      "|      6|2023-12-31 16:46:...|  null|    null|VACUUM START|{retentionCheckEn...|null|    null|     null|          5|SnapshotIsolation|         true|{numFilesToDelete...|        null|Apache-Spark/3.4....|\n",
      "|      5|2023-12-31 16:14:...|  null|    null|     RESTORE|{version -> 1, ti...|null|    null|     null|          4|     Serializable|        false|{numRestoredFiles...|        null|Apache-Spark/3.4....|\n",
      "|      4|2023-12-31 16:12:...|  null|    null|     RESTORE|{version -> 2, ti...|null|    null|     null|          3|     Serializable|        false|{numRestoredFiles...|        null|Apache-Spark/3.4....|\n",
      "|      3|2023-12-31 16:10:...|  null|    null|     RESTORE|{version -> 0, ti...|null|    null|     null|          2|     Serializable|        false|{numRestoredFiles...|        null|Apache-Spark/3.4....|\n",
      "|      2|2023-12-29 21:42:...|  null|    null|     RESTORE|{version -> 0, ti...|null|    null|     null|          1|     Serializable|        false|{numRestoredFiles...|        null|Apache-Spark/3.4....|\n",
      "|      1|2023-12-29 14:25:...|  null|    null|       MERGE|{predicate -> (tg...|null|    null|     null|          0|     Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.2....|\n",
      "|      0|2023-12-29 14:25:...|  null|    null|CREATE TABLE|{isManaged -> fal...|null|    null|     null|       null|SnapshotIsolation|         true|                  {}|        null|Apache-Spark/3.2....|\n",
      "+-------+--------------------+------+--------+------------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fullHistoryDF = deltaTable.history()\n",
    "fullHistoryDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b294399-02e1-46b8-8abb-22546857a6ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e63d694-1cd9-4d74-a59c-5aa870b441e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
