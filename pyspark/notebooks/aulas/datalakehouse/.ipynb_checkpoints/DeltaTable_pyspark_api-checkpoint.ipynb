{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7887f040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, expr\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType, IntegerType, ArrayType, DateType\n",
    "import sys\n",
    "import os\n",
    "from delta import DeltaTable\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from delta.tables import *\n",
    "import io\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e979b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark_packages_list = [\n",
    "            'io.delta:delta-core_2.12:1.1.0',\n",
    "            'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0',\n",
    "            'org.mongodb.spark:mongo-spark-connector:10.0.2'\n",
    "        ]\n",
    "    warehouse_location = '/mnt/datalake/warehouse'\n",
    "    spark_packages = \",\".join(spark_packages_list)\n",
    "#     derby_location = '/mnt/datalake/derby'\n",
    "#     .config(\"spark.driver.extraJavaOptions\", f\"Dderby.system.home='{derby_location}'\") \\\n",
    "    return SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"File Streaming Demo\") \\\n",
    "        .master(\"local[3]\") \\\n",
    "        .config(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "        .config(\"spark.jars.packages\", spark_packages) \\\n",
    "        .config(\"spark.streaming.stopGracefullyOnShutdown\", \"true\") \\\n",
    "        .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.1.0\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "\n",
    "#        .config(\"spark.driver.extraJavaOptions\", f\"Dderby.system.home='{derby_location}'\") \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06b6ccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef1368bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([ \n",
    "    StructField(\"product_id\",IntegerType(),True), \\\n",
    "    StructField(\"product_name\",StringType(),True),\n",
    "    StructField(\"product_category_name\",StringType(),True), \\\n",
    "    StructField(\"product_price\", DoubleType(), True) \n",
    "  ])\n",
    "\n",
    "data_product = [(1,'perfurme magico','perfumaria',100.5),\n",
    "                (2,'replica vaso ming','artes',55.75),\n",
    "               (3,'raquete de tenis','esporte_lazer',350.00),\n",
    "                (4,'mordedor','bebes',27.25),\n",
    "                (5,'televisor 46 polegadas 4k','utilidades_domesticas',2555.55)\n",
    "               ]\n",
    "\n",
    "\n",
    "df_products = spark.createDataFrame(data=data_product,schema=schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b530b617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------------------+-------------+\n",
      "|product_id|        product_name|product_category_name|product_price|\n",
      "+----------+--------------------+---------------------+-------------+\n",
      "|         1|     perfurme magico|           perfumaria|        100.5|\n",
      "|         2|   replica vaso ming|                artes|        55.75|\n",
      "|         3|    raquete de tenis|        esporte_lazer|        350.0|\n",
      "|         4|            mordedor|                bebes|        27.25|\n",
      "|         5|televisor 46 pole...| utilidades_domest...|      2555.55|\n",
      "+----------+--------------------+---------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_products.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec2bc21",
   "metadata": {},
   "source": [
    "### Criando delta table utilizando write\n",
    "A forma mais simples de criar uma Delta Table informando o format \"delta\", como abaixo. <br>\n",
    "Abaixo uma Delta Table é criada e os dados podem ser sobrescritos ou acrescentados, <br> \n",
    "utilizando respectivamente as opções\n",
    "mode **OVERWRITE** ou **APPEND**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab75048",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_products = '/mnt/datalake/bronze/test/products/'\n",
    "mode = 'OVERWRITE'\n",
    "df_products\\\n",
    ".write.format(\"delta\")\\\n",
    ".option(\"overwriteSchema\", \"true\") \\\n",
    ".save(path_products, mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3c67ef",
   "metadata": {},
   "source": [
    "### Carregando os dados escritos como delta table \n",
    "Para a utilização dos metodos do Delta Table, os dados devem ser carregados como Delta Tables <br>\n",
    "atravez do método **DeltaTable.forPath(spark, caminho_Delta_table)**\n",
    "\n",
    "**Importante** Uma DeltaTable não é um spark dataframe, então caso seja necessário utilizar operações e ações de um <br> \n",
    "spark dataframe existem duas formas :\n",
    "* Carregar os dados gravados no formato delta para um dataframe pyspark,utilizar o spark.read.format('delta').load(caminho_Delta_table) \n",
    "\n",
    "* Carregar uma DeltaTable com **DeltaTable.forPath(spark, caminho_Delta_table)** e usar o metodo **toDf()** , que retorna um dataframe spark de uma tabela delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cd452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_products = '/mnt/datalake/bronze/test/products/'\n",
    "df_products_read = spark.read.format('delta').load(path_products)\n",
    "df_products_read.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02707139",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_tb  = '/mnt/datalake/bronze/test/products/'\n",
    "deltaTable = DeltaTable.forPath(spark, location_tb)\n",
    "df_from_delta = deltaTable.toDF()\n",
    "df_from_delta.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff16bfc",
   "metadata": {},
   "source": [
    "### Apagando dados delta table \n",
    "Com DeltaTables é possível remover dados a nivel de linha, sem ser necessário sobrescrever os dados . <br>\n",
    "Carregue uma DeltaTable com **DeltaTable.forPath(spark, location_tb)** e utilize o método delete para informar a <br> \n",
    "condição de remoção. <br>\n",
    "Uma String com uma condição SQL pode ser utilizada ou a notação spark.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92637d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# condição no formato sql.\n",
    "deltaTable.delete(\"product_id = 5\")\n",
    "\n",
    "# condição no formato spark\n",
    "deltaTable.delete(col('product_id') == 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1666d6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_delta.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566f31a5",
   "metadata": {},
   "source": [
    "### Alterando dados delta table \n",
    "Com DeltaTables é possível alterar dados a nivel de linha, sem ser necessário sobrescrever os dados . <br>\n",
    "Carregue uma DeltaTable com **DeltaTable.forPath(spark, location_tb)** e utilize o update delete para informar a <br> \n",
    "condição de remoção. <br>\n",
    "Uma String com uma condição SQL pode ser utilizada ou a notação spark. <br>\n",
    "O método update utiliza dois parametros, um para a condição de update e outro para informar quais campos serão alterados <br> e como serão alterados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e039f187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# condição no formato sql.\n",
    "deltaTable.update(\n",
    "  condition = \"product_id = 1\",\n",
    "  set = { \"product_name\": \"'perfurme magico cheiroso'\" }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850cfeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# condição formato spark , utilizando uma função spark para informar o nome da coluna\n",
    "deltaTable.update(\n",
    "  condition = col('product_id') == 1,\n",
    "  set = { 'product_price': col('product_price') * 1.2 }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5c38b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_delta.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61271e61",
   "metadata": {},
   "source": [
    "### Utilizando MERGE para inserir, alterar e remover dados de uma Delta Table\n",
    "Outra forma de atualizar, remover e inserir dados numa DeltaTable é com **MERGE** <br>\n",
    "Um metodo MERGE abaixo se assemelha ao exemplo abaixo <br>\n",
    "\n",
    "<br>\n",
    "deltaTable.alias('tgt') \\ <br>\n",
    ".merge( <br>\n",
    "    df_products.alias('src'), <br>\n",
    "    \"tgt.product_id = src.product_id\" <br>\n",
    ") \\ <br> \n",
    ".whenMatchedUpdateAll() \\ <br>\n",
    ".whenNotMatchedInsertAll() \\ <br>\n",
    ".execute() <br>\n",
    "<br>\n",
    "\n",
    "O metodo merge atualiza uma DeltaTable existente utilizando dados de um spark dataframe, onde \n",
    "o DeltaTable é comparado com o spark dataframe utilizando uma coluna que funciona como uma **Chave Unica (Sem repetição)** ,\n",
    "apartir do resultado da comparação eventos de inserção e atualização são disparados. Os eventos mais comuns são : <br>\n",
    "* **whenMatchedUpdateAll()** quando a chave for encontrada, todas os campos das linhas que atendam a condição da DeltaTable são atualizadas com o conteudo do dataframe spark <br>\n",
    "* **whenNotMatchedInsertAll()** quando a chave não for encontrada, todas as linhas que atendam a condição do dataframe spark são inseridas no DeltaTable <br> \n",
    "* **whenMatchedUpdate**  quando a chave for encontrada, os campos informados das linhas que atendam a condição da DeltaTable são atualizadas com o conteudo do dataframe spark<br>\n",
    "* **whenNotMatchedInsert** quando a chave não for encontrada, Os campos informados de todas as linhas que atendam a condição do dataframe spark são inseridas no DeltaTable <br> \n",
    "* **whenMatchedDelete** quando a condição for satisfeita, todas as linhas que atendam a condição são removidas do DeltaTable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8ed0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([ \n",
    "    StructField(\"product_id\",IntegerType(),True), \\\n",
    "    StructField(\"product_name\",StringType(),True),\n",
    "    StructField(\"product_category_name\",StringType(),True), \\\n",
    "    StructField(\"product_price\", DoubleType(), True) \n",
    "  ])\n",
    "\n",
    "data_product = [(2,'replica vaso ming importado','artes',70.75),\n",
    "                (3,'raquete de tenis nacional','esporte_lazer',365.00),\n",
    "                (4,'mordedor','bebes',27.25),\n",
    "                (5,'televisor 46 polegadas 4k','utilidades_domesticas',2555.55),\n",
    "                (6,'PS','utilidades_domesticas',2555.55)\n",
    "               ]\n",
    "\n",
    "\n",
    "df_products = spark.createDataFrame(data=data_product,schema=schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845bef68",
   "metadata": {},
   "source": [
    "Abaixo o método MERGE é utilizado numa DeltaTable, onde a inserção e atualização é realizada somente com os campos especificos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae5132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable.alias(\"target\")\\\n",
    ".merge(\n",
    "    source = df_products.alias(\"source\"),\n",
    "    condition = \"source.product_id = target.product_id\"\n",
    ").whenMatchedUpdate(\n",
    "    set =\n",
    "        {\n",
    "          \"target.product_name\": \"source.product_name\",\n",
    "          \"target.product_category_name\": \"source.product_category_name\"\n",
    "        }\n",
    ").whenNotMatchedInsert(\n",
    "    values =\n",
    "        {\n",
    "          \"target.product_name\": \"source.product_price\",\n",
    "          \"target.product_category_name\": \"source.product_category_name\",\n",
    "          \"target.product_name\": \"source.product_name\",\n",
    "          \"target.product_price\": \"source.product_price\"\n",
    "        }\n",
    ").execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27bed29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938f336e",
   "metadata": {},
   "source": [
    "No utiliza ção anterior, durante o metodo **whenNotMatchedInsert**, o campo product_id não foi informando, resultado <br>na inserção com o valor nullo na DeltaTable, como esse campo é utilizado como chave, erros podem ocorrer, o ideal é remover a linha. <br>\n",
    "Abaixo o método **MERGE** é utilizado numa DeltaTable em conjunto com **whenMatchedDelete**, para remover as linhas onde o campo product_id seja null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4681f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Usando merge com delete\n",
    "deltaTable.alias(\"target\")\\\n",
    ".merge(\n",
    "    source = df_products.alias(\"source\"),\n",
    "    condition = \"target.product_id is null\"\n",
    ").whenMatchedDelete()\\\n",
    ".execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4616ea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a005b165",
   "metadata": {},
   "source": [
    "Abaixo o método **MERGE** é utilizado numa DeltaTable em conjunto com **whenMatchedUpdateAll** e **whenNotMatchedInsertAll**, para atualizar todas as linhas encontradas no dataframe e DeltaTable ou inserir todas as linhas não encontradas <br>\n",
    "Os dois metodos consideram que todos os campos são atualizados ou inseridos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2cb6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_condition = \"tgt.product_id = src.product_id\"\n",
    "deltaTable.alias('tgt') \\\n",
    ".merge(\n",
    "    df_products.alias('src'),\n",
    "    merge_condition\n",
    ") \\\n",
    ".whenMatchedUpdateAll() \\\n",
    ".whenNotMatchedInsertAll() \\\n",
    ".execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dedbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ad14ec",
   "metadata": {},
   "source": [
    "### Modificando o Schema de uma DeltaTable existente\n",
    "É possivel modificar o schema de uma DeltaTable existente, adicionando a configuração <br> **config(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")** na sparkSession. <br>\n",
    "No exemplo abaixo, a nova coluna **status** foi adicionado ao Dataframe, para os registros antigos, o valor do campo será null .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24d138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([ \n",
    "    StructField(\"product_id\",IntegerType(),True), \\\n",
    "    StructField(\"product_name\",StringType(),True),\n",
    "    StructField(\"product_category_name\",StringType(),True), \\\n",
    "    StructField(\"product_price\", DoubleType(), True),\n",
    "    StructField(\"status\",StringType(),True),\n",
    "  ])\n",
    "\n",
    "data_product = [(2,'replica vaso ming importado','artes',70.75,'Active'),\n",
    "                (3,'raquete de tenis nacional','esporte_lazer',365.00,'Active'),\n",
    "                (4,'mordedor','bebes',27.25,'Inative'),\n",
    "                (5,'televisor 46 polegadas 4k','utilidades_domesticas',2555.55,'Active'),\n",
    "                (6,'PS','utilidades_domesticas',2555.55,'Active')\n",
    "               ]\n",
    "\n",
    "\n",
    "df_products = spark.createDataFrame(data=data_product,schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bee904",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_condition = \"tgt.product_id = src.product_id\"\n",
    "deltaTable.alias('tgt') \\\n",
    ".merge(\n",
    "    df_products.alias('src'),\n",
    "    merge_condition\n",
    ") \\\n",
    ".whenMatchedUpdateAll() \\\n",
    ".whenNotMatchedInsertAll() \\\n",
    ".execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de257b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_delta.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf0c1ad",
   "metadata": {},
   "source": [
    "### Criando uma DeltaTable utilizando create\n",
    "Outra forma de criar uma DeltaTable é utilizando o metodo **create** , onde os campos e local de escrita são informados.\n",
    "O método create cria a DeltaTable vazia, possibilitando a utilização de métodos como o **merge** , **update** e **delete** . <br>\n",
    "Essa é uma forma de criação bastante interessante para a criação de métodos inserção e atualização mais genéricos. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bbc9f6",
   "metadata": {},
   "source": [
    "Abaixo o codigo realiza as tarefas\n",
    "* criando a lista das colunas utilizadas\n",
    "* verifica se existe uma DeltaTable no diretório informado, caso positivo, executa um MERGE para atualização/inserção\n",
    "* verifica se existe uma DeltaTable no diretório informado, caso negativo, cria uma DeltaTable com a estrutura informada e executa um MERGE para atualização/inserção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efd92ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "False\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Table delta.`/mnt/datalake/bronze/test/products/` already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-36e12b750df7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDeltaTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misDeltaTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mDeltaTable\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0maddColumns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/delta/tables.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1165\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mnote\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mEvolving\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \"\"\"\n\u001b[0;32m-> 1167\u001b[0;31m         \u001b[0mjdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDeltaTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table delta.`/mnt/datalake/bronze/test/products/` already exists"
     ]
    }
   ],
   "source": [
    "\n",
    "location_tb  = '/mnt/datalake/bronze/test/products/'\n",
    "merge_condition = \"tgt.product_id = src.product_id\"\n",
    "\n",
    "columns = [\n",
    "    StructField('product_id', IntegerType(), True),\n",
    "    StructField('product_name', StringType(), True),\n",
    "    StructField('product_category_name', StringType(), True),\n",
    "    StructField('product_price', DoubleType(), True),\n",
    "    StructField('status', StringType(), True)\n",
    "]\n",
    "\n",
    "if (DeltaTable.isDeltaTable(spark, location_tb)):\n",
    "    print('1')\n",
    "    deltaTable = DeltaTable.forPath(spark, location_tb)\n",
    "    deltaTable.alias('tgt') \\\n",
    "        .merge(\n",
    "            df_products.alias('src'),\n",
    "            merge_condition\n",
    "        ) \\\n",
    "        .whenMatchedUpdateAll() \\\n",
    "        .whenNotMatchedInsertAll() \\\n",
    "        .execute()\n",
    "else:\n",
    "    print('2')\n",
    "    print(DeltaTable.isDeltaTable(spark, location_tb))\n",
    "    DeltaTable \\\n",
    "            .create(spark) \\\n",
    "            .addColumns(columns) \\\n",
    "            .location(location_tb) \\\n",
    "            .execute()\n",
    "\n",
    "    deltaTable = DeltaTable.forPath(spark, location_tb)\n",
    "    deltaTable.alias('tgt') \\\n",
    "        .merge(\n",
    "            df_products.alias('src'),\n",
    "            merge_condition\n",
    "        ) \\\n",
    "        .whenMatchedUpdateAll() \\\n",
    "        .whenNotMatchedInsertAll() \\\n",
    "        .execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a793f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aedee16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /mnt/datalake/bronze/test/products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887aa7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b01d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ebf01b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
