{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "def11a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, expr\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType, IntegerType, ArrayType, DateType\n",
    "import sys\n",
    "import os\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from delta.tables import *\n",
    "import io\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3aa0ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark_packages_list = [\n",
    "            'io.delta:delta-core_2.12:2.4.0',\n",
    "            'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0',\n",
    "            'org.mongodb.spark:mongo-spark-connector:10.0.2'\n",
    "        ]\n",
    "    warehouse_location = '/mnt/datalake/warehouse'\n",
    "    #bin/spark-sql --packages io.delta:delta-core_2.12:2.4.0 --conf \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\" --conf \"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\"\n",
    "\n",
    "    spark_packages = \",\".join(spark_packages_list)\n",
    "#     derby_location = '/mnt/datalake/derby'\n",
    "#     .config(\"spark.driver.extraJavaOptions\", f\"Dderby.system.home='{derby_location}'\") \\\n",
    "    return SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"File Streaming Demo\") \\\n",
    "        .master(\"local[3]\") \\\n",
    "        .config(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "        .config(\"spark.jars.packages\", spark_packages) \\\n",
    "        .config(\"spark.streaming.stopGracefullyOnShutdown\", \"true\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"delta.deletedFileRetentionDuration\",7)\\\n",
    "        .config(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"false\") \\\n",
    "        .config(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\") \\\n",
    "        .config(\"spark.sql.legacy.allowNonEmptyLocationInCTAS\",\"true\")\\\n",
    "        .config(\"hive.stats.jdbc.timeout\", 30) \\\n",
    "        .config(\"hive.stats.retries.wait\", 3000) \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c7f6cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.stats.retries.wait\n",
      "Warning: Ignoring non-Spark config property: hive.stats.jdbc.timeout\n",
      "Warning: Ignoring non-Spark config property: delta.deletedFileRetentionDuration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.4.0-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-2b90c6c1-46de-4efc-a149-b127b84419cd;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.4.0 in central\n",
      "\tfound io.delta#delta-storage;2.4.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound org.mongodb.spark#mongo-spark-connector;10.0.2 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.5.1 in central\n",
      "\t[4.5.1] org.mongodb#mongodb-driver-sync;[4.5.0,4.5.99)\n",
      "\tfound org.mongodb#bson;4.5.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.5.1 in central\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-core_2.12/2.4.0/delta-core_2.12-2.4.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-core_2.12;2.4.0!delta-core_2.12.jar (3124ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.2.0/spark-sql-kafka-0-10_2.12-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0!spark-sql-kafka-0-10_2.12.jar (599ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector/10.0.2/mongo-spark-connector-10.0.2.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb.spark#mongo-spark-connector;10.0.2!mongo-spark-connector.jar (366ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/2.4.0/delta-storage-2.4.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;2.4.0!delta-storage.jar (382ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.9.3/antlr4-runtime-4.9.3.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.9.3!antlr4-runtime.jar (564ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.2.0/spark-token-provider-kafka-0-10_2.12-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0!spark-token-provider-kafka-0-10_2.12.jar (468ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.8.0/kafka-clients-2.8.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;2.8.0!kafka-clients.jar (2310ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (345ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.6.2/commons-pool2-2.6.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.6.2!commons-pool2.jar (418ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (385ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.1/hadoop-client-runtime-3.3.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.1!hadoop-client-runtime.jar (5401ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.7.1/lz4-java-1.7.1.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.7.1!lz4-java.jar (493ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.8.4/snappy-java-1.1.8.4.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.8.4!snappy-java.jar(bundle) (662ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.30!slf4j-api.jar (302ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.1/hadoop-client-api-3.3.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.1!hadoop-client-api.jar (2468ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/htrace/htrace-core4/4.1.0-incubating/htrace-core4-4.1.0-incubating.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.htrace#htrace-core4;4.1.0-incubating!htrace-core4.jar (532ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...\n",
      "\t[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (375ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/4.5.1/mongodb-driver-sync-4.5.1.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#mongodb-driver-sync;4.5.1!mongodb-driver-sync.jar (417ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/bson/4.5.1/bson-4.5.1.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#bson;4.5.1!bson.jar (434ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-core/4.5.1/mongodb-driver-core-4.5.1.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#mongodb-driver-core;4.5.1!mongodb-driver-core.jar (544ms)\n",
      ":: resolution report :: resolve 24618ms :: artifacts dl 20633ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.delta#delta-core_2.12;2.4.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.mongodb#bson;4.5.1 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.5.1 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.5.1 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector;10.0.2 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   20  |   20  |   20  |   0   ||   20  |   20  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-2b90c6c1-46de-4efc-a149-b127b84419cd\n",
      "\tconfs: [default]\n",
      "\t20 artifacts copied, 0 already retrieved (65996kB/77ms)\n",
      "24/01/19 23:35:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/19 23:35:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/01/19 23:35:20 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/01/19 23:35:20 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "24/01/19 23:35:20 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdf819e-682d-4a86-a223-31c16fb407c2",
   "metadata": {},
   "source": [
    "O catálogo de metadados do Spark pode ser acessado pelo objeto\n",
    "\n",
    "`SparkSession.catalog` \n",
    "\n",
    "As principais funcionalidades são:\n",
    "\n",
    "* `listDatabases()`: lista todas os databases disponíveis;\n",
    "* `listTables()`: lista todas as tabelas disponíveis em um determinado database;\n",
    "* `listFucntions()`: lista as funções disponíveis em um determinado database;\n",
    "* `refreshTable()`: atualiza os metadados de uma determinada tabela\n",
    "* `uncacheTable()`: remove uma tabela salva em memória\n",
    "* `clearCache()`: remove todas as tabelas salvas em memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ffc277b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/19 23:35:31 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/01/19 23:35:31 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/01/19 23:36:05 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "24/01/19 23:36:05 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore jovyan@172.22.0.2\n",
      "24/01/19 23:36:05 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Database(name='default', catalog='spark_catalog', description='Default Hive database', locationUri='file:/mnt/datalake/warehouse')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ef5b856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/19 23:36:12 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c36434-8215-472e-a83a-36d52544dd02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3224c9af-0da1-4035-aa21-4243de51c463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa1402e1-e417-42eb-afc6-3f650a6fded2",
   "metadata": {},
   "source": [
    "### Show e Create Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310a9853-a05b-4da8-b3f9-4f7b31cec18e",
   "metadata": {},
   "source": [
    "Os databases do Spark são uma ferramenta para organizar tabelas. Eles podem e devem ser vistos como algo muito próximo dos databases de servidores de bancos de dados relacionais. O Spark utiliza por padrão um database chamado default, que serve para criar tabelas, views e realizar consultas caso o usuário não tenha definido o seu próprio. Um ponto importante é que essas estruturas persistem em diferentes sessões: se o usuário mudar de database, todas as tabelas permanecerão no database anterior e vão precisar ser consultadas de maneira diferente.\n",
    "\n",
    "Existem alguns comandos do SQL importantes na hora de se trabalhar com databases. Else são:\n",
    "\n",
    "* `SHOW DATABASES`: lista todas os databases disponíveis, de forma análoga ao Catalog ;\n",
    "* `CREATE DATABASE <nome_do_db>`: cria um database\n",
    "* `USE <nome_do_db>`: define o database como o atual para a realização de queries\n",
    "    * **Obs**: ao se mudar de database, é possível acessar tabelas de um database anterior usando o prefixo “nome_do_db.” antes do nome da tabela. Exemplo:\n",
    "        ```\n",
    "        USE db2\n",
    "        SELECT * FROM db1.table\n",
    "        ```\n",
    "* `SELECT current_database()`: retorna qual o database definido como o atual\n",
    "* `DROP DATABASE IF EXISTS <nome_do_db>`: deleta determinado database dentre aqueles que foram definidos. Atenção: nunca delete o database default do Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afccdad-4809-4e54-ba9d-887f5e3dfa7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dea29d8-a7c9-4e2b-8ae9-07973097ba60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/19 23:36:13 WARN ObjectStore: Failed to get database department, returning NoSuchObjectException\n",
      "24/01/19 23:36:13 WARN ObjectStore: Failed to get database department, returning NoSuchObjectException\n",
      "24/01/19 23:36:13 WARN ObjectStore: Failed to get database department, returning NoSuchObjectException\n",
      "chgrp: changing ownership of 'file:///mnt/datalake/warehouse/department.db': chown: changing group of '/mnt/datalake/warehouse/department.db': Operation not permitted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE DATABASE IF NOT EXISTS department;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "509b327a-5114-450a-9145-e7109d17515b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', catalog='spark_catalog', description='Default Hive database', locationUri='file:/mnt/datalake/warehouse'),\n",
       " Database(name='department', catalog='spark_catalog', description='', locationUri='file:/mnt/datalake/warehouse/department.db')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59b49994-1a5a-49de-8061-4be786df099f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables('department')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3373e488-0e6f-41ba-9657-d2da9938410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_countries = '../../datalake/transient/departments/countries/'\n",
    "df_countries = spark.read.format('csv')\\\n",
    ".option(\"header\", True)\\\n",
    ".option(\"sep\", \",\")\\\n",
    ".option(\"quote\",\"\\'\")\\\n",
    ".option(\"inferSchema\",True)\\\n",
    ".load(path_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "661acf28-acb8-44d9-8e94-f62084bc276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_regions = '../../datalake/transient/departments/regions/'\n",
    "df_regions = spark.read.format('csv')\\\n",
    ".option(\"header\", True)\\\n",
    ".option(\"sep\", \",\")\\\n",
    ".option(\"quote\",\"\\'\")\\\n",
    ".option(\"inferSchema\",True)\\\n",
    ".load(path_regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e76bd24-a5d6-4e61-8716-384657a8a47e",
   "metadata": {},
   "source": [
    "### Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e312480f-305d-4400-a38c-d79de648dfa5",
   "metadata": {},
   "source": [
    "* **Managed Tables**: o Spark administra tanto os dados quanto os metadados das tabelas, de forma que operações como DROP TABLE afetam também os dados escritos em disco;\n",
    "* **Unmanaged Tables**: o Spark administra somente os metadados da tabela, e os dados escritos em disco não são alterados em nenhum momento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c43fe57-6f35-4eff-b23b-499ac0ca5671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "USE department;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "378c47ea-347a-4480-9b32-26636f4c90ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables('department')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb189fa-d07a-4682-bcfb-b1e601072c11",
   "metadata": {},
   "source": [
    "**Criando Unmanaged Tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4eb9aa85-d7d1-4388-95ff-74b6449432ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/19 23:39:09 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "24/01/19 23:39:09 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "24/01/19 23:39:09 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/01/19 23:39:09 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "chgrp: changing ownership of 'file:///mnt/datalake/datalake': chown: changing group of '/mnt/datalake/datalake': Operation not permitted\n"
     ]
    }
   ],
   "source": [
    "df_regions.write.option('path', '../../datalake/bronze/sql_db/departments/regions').saveAsTable(\"regions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc91b0e0-1b1a-4a5f-9ba9-191a1a19d844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='regions', catalog='spark_catalog', namespace=['department'], description=None, tableType='EXTERNAL', isTemporary=False)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables('department')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2a08998-bcaa-4f09-9018-24d99bdc80a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|region_id|region_name|\n",
      "+---------+-----------+\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select * from regions limit 5;\n",
    "\"\"\").show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518549e5-860b-4063-b2fe-2e3f6df2a883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "311f906a-c276-4231-b14c-a1f1f0f38fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "DROP TABLE regions;\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59d68176-db31-4f47-85f5-6778c1127f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables('department')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91b1098b-5422-4f22-baf7-65b33cc701e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 20\n",
      "drwxr-xr-x 2 jovyan users 4096 Jan 19 23:39 .\n",
      "drwxr-xr-x 3 jovyan users 4096 Jan 19 23:39 ..\n",
      "-rw-r--r-- 1 jovyan users  823 Jan 19 23:39 part-00000-cf1ce8a0-9a75-4327-95bd-b42cba29349e-c000.snappy.parquet\n",
      "-rw-r--r-- 1 jovyan users   16 Jan 19 23:39 .part-00000-cf1ce8a0-9a75-4327-95bd-b42cba29349e-c000.snappy.parquet.crc\n",
      "-rw-r--r-- 1 jovyan users    0 Jan 19 23:39 _SUCCESS\n",
      "-rw-r--r-- 1 jovyan users    8 Jan 19 23:39 ._SUCCESS.crc\n"
     ]
    }
   ],
   "source": [
    "! ls -la ../../datalake/bronze/sql_db/departments/regions/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "111eed23-2eb0-4163-8df7-712a6f0b9326",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `regions` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 2 pos 14;\n'GlobalLimit 5\n+- 'LocalLimit 5\n   +- 'Project [*]\n      +- 'UnresolvedRelation [regions], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;43mselect * from regions limit 5;\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1439\u001b[0m     litArgs \u001b[38;5;241m=\u001b[39m {k: _to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `regions` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 2 pos 14;\n'GlobalLimit 5\n+- 'LocalLimit 5\n   +- 'Project [*]\n      +- 'UnresolvedRelation [regions], [], false\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select * from regions limit 5;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb71c69d-d5bf-42d4-b153-99de773d8b39",
   "metadata": {},
   "source": [
    "**Criando Managed Tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b54e67e6-453b-465c-9861-d09453ddaf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regions.write.saveAsTable(\"regions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "450c76fe-0034-495f-bfbb-c2e2e0fad903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 20\n",
      "drwxr-xr-x 2 jovyan users 4096 Jan 19 23:40 .\n",
      "drwxr-xr-x 3 jovyan users 4096 Jan 19 23:40 ..\n",
      "-rw-r--r-- 1 jovyan users  823 Jan 19 23:40 part-00000-f89201d6-0120-467c-aeed-c07e66df8de0-c000.snappy.parquet\n",
      "-rw-r--r-- 1 jovyan users   16 Jan 19 23:40 .part-00000-f89201d6-0120-467c-aeed-c07e66df8de0-c000.snappy.parquet.crc\n",
      "-rw-r--r-- 1 jovyan users    0 Jan 19 23:40 _SUCCESS\n",
      "-rw-r--r-- 1 jovyan users    8 Jan 19 23:40 ._SUCCESS.crc\n"
     ]
    }
   ],
   "source": [
    "! ls -la ../../datalake/warehouse/department.db/regions/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d1879d8-aa9a-4b32-be9a-4d854f81c9f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "DROP TABLE regions;\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "146c172f-ef8a-4122-aa5b-349dfa65978b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '../../datalake/warehouse/department.db/regions': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! ls -la ../../datalake/warehouse/department.db/regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71cf0b7f-1baa-4fe4-b50d-66eaead44f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country_id: string (nullable = true)\n",
      " |-- country_name: string (nullable = true)\n",
      " |-- region_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_countries.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cfe962-3cd0-4cc0-8c03-d64c76c210d2",
   "metadata": {},
   "source": [
    "**Criando Views**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "624179cf-407e-484d-acea-76b3bf030b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_countries.createOrReplaceTempView('countries_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d2a4fc9-25fc-4c15-ba35-0edfee633021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---------+\n",
      "|country_id|country_name|region_id|\n",
      "+----------+------------+---------+\n",
      "|        AR|   Argentina|        2|\n",
      "|        AU|   Australia|        3|\n",
      "|        BE|     Belgium|        1|\n",
      "|        BR|      Brazil|        2|\n",
      "|        CA|      Canada|        2|\n",
      "|        CH| Switzerland|        1|\n",
      "|        CN|       China|        3|\n",
      "|        DE|     Germany|        1|\n",
      "|        DK|     Denmark|        1|\n",
      "|        EG|       Egypt|        4|\n",
      "|        FR|      France|        1|\n",
      "|        HK|    HongKong|        3|\n",
      "|        IL|      Israel|        4|\n",
      "|        IN|       India|        3|\n",
      "|        IT|       Italy|        1|\n",
      "|        JP|       Japan|        3|\n",
      "|        KW|      Kuwait|        4|\n",
      "|        MX|      Mexico|        2|\n",
      "|        NG|     Nigeria|        4|\n",
      "|        NL| Netherlands|        1|\n",
      "+----------+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select * from countries_view;\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8fab4cd-66e8-4599-8be1-c105c3972fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_countries.createOrReplaceGlobalTempView('countries_global_view')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d736c957-1906-4419-8f99-f3d5efe63fee",
   "metadata": {},
   "source": [
    "**Utilizando a interface SQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33d3bf1c-f038-4c89-b2fe-2b5b694f1ac8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `spark_catalog`.`department`.`countries` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;43m  DROP TABLE countries ;\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1439\u001b[0m     litArgs \u001b[38;5;241m=\u001b[39m {k: _to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `spark_catalog`.`department`.`countries` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS."
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "  DROP TABLE countries ;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b21bc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "  CREATE TABLE countries (\n",
    "  country_id STRING, \n",
    "  country_name STRING,\n",
    "  region_id INTEGER\n",
    ") \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad884ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listTables('department')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fb4802",
   "metadata": {},
   "outputs": [],
   "source": [
    "### inserindo registros na tabela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc76cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" \n",
    "            insert into department.countries values\n",
    "            ('100','Never Land',99)\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fc8e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM department.countries limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a98a538-9d49-450f-ad98-ec28021d7e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" \n",
    "        insert into countries\n",
    "        select        \n",
    "              country_id , \n",
    "              country_name ,       \n",
    "              region_id      \n",
    "        from countries_view\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcd703b-a69d-41f8-b4b2-e10657466cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM department.countries limit 50\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d52100-95f7-4692-9f23-d064d5371d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('SHOW TABLES;').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15028b6a-8744-425b-a71d-7d766191c98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56519f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listTables('department')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5780787b-cd59-4da2-ab0c-fc0341a008d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
