{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f39aede2-c336-4ac7-bb19-6268a10f920d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d8a00ec-a322-4d9f-ae93-7fb9cc63f094",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **Spark Session** ###\n",
    "A Spark Session é o ponto de entrada pra acessar todas as funcionalidades do Spark. Por meio dela, é possível: <br>\n",
    "• **Ler e criar DataFrames;<br>\n",
    "• Realizar queries do SQL;<br>\n",
    "• Configurar a aplicação;<br>\n",
    "• Acessar o catálogo de metadados.**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21ba4246-a05f-499c-9512-48572dd49dc5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **Metodos mais comuns** ###\n",
    "\n",
    "- version() – Returns the Spark version where your application is running, probably the Spark version your cluster is configured with. <br>\n",
    "\n",
    "- createDataFrame() – Cria um Dataframe de uma coleção  <br>\n",
    "\n",
    "- getActiveSession() – Retorna a SparkSession ativa ativa <br>\n",
    "\n",
    "- read() – Usado para ler registros de csv, parquet, avro, delta entre outros formatos num DataFrame. <br>\n",
    "\n",
    "- readStream() – Usado para ler dados streaming num DataFrame. <br>\n",
    "\n",
    "- sparkContext() – Retorna um SparkContext. <br>\n",
    "\n",
    "- sql() – Retorna um Dataframe equivalente a execução de uma instrução SQL. <br>\n",
    "\n",
    "- sqlContext() – Retorna um SQLContext. <br>\n",
    "\n",
    "- stop() – Para o SparkContext atual. <br>\n",
    "\n",
    "- table() – Retorna um Dataframe de uma tabela ou view. <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6ea1391-7c4c-48b5-933c-6ab4b70131d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **Criando uma Spark Session** ###\n",
    "Algumas formas de criar uma sparkSession: <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afb01c0c-c307-4d4f-bc09-3868bbae30c4",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1daa1988-e90d-4a6a-b4a8-713350848348",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Adicionando configurações a sparkSession durante a criação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57b95ff4-2d77-4c8f-b151-da3e5206575e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    ".config('spark.executor.memory', '8G')\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34ca1f46-76c1-441a-a1a0-1d65ab9a2087",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Verificando o valor de alguma configuração no spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b864e12e-2a63-4520-bb48-300196c40930",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "partitions = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "print(partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e2f6174-6bbb-493b-b7af-413f73ef70e8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **Criando uma Spark Context** ###\n",
    "Desde o spark 2.0 , a nova forma de acessar driver do spark é com uma SparkSession, atravez da SparkSession temos  <br>\n",
    "Acesso ao SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a463bdd3-0bb7-4bf7-ab80-93bb6a6a45c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sparkContext=spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a1a88e0-e549-40e2-82ae-f4439dc2a7b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **Criando um RDD** ###\n",
    "Utilizando o metodo parallelize, podemos criar um RDD , passando uma lista como fonte de dados, por exemplo.<br>\n",
    "Com o RDD criado, podemos realizar uma série de ações e transformações . Abaixo executaremos algumas demonstrações de utilização.\n",
    "### **Metodos mais comuns de um RDD** ###\n",
    "\n",
    "- count() – Retorna a quantidade de elementos do RDD <br>\n",
    "\n",
    "- collect() – Retorna uma lista com os elementos do RDD  <br>\n",
    "\n",
    "- foreach(f) – Executa uma função para cada elemento do RDD, recebe como parametro uma função <br>\n",
    "\n",
    "- filter(f) – Retorna um novo RDD que satisfaça a função dentro do filter. <br>\n",
    "\n",
    "- map(f) – Retorna um novo RDD aplicando a função para cada elemento. <br>\n",
    "\n",
    "- join() – Retorna um novo RDD com os elementos que correspondam a uma chave . <br>\n",
    "Esse metodo é utilizado para executar uma junção entre dois RDD's <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec1640b3-6e12-41c8-86fc-e73e0b8413d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "words = [\"scale\",\"Java\",\"Hadoop\",\"spark\",\"Akka\",\"spark vs Hadoop\",\"pyspark\",\"pyspark and spark\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5373e8ac-2c57-4e99-817a-ead41f02a7ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: pyspark.rdd.RDD"
     ]
    }
   ],
   "source": [
    "words_rdd = sparkContext.parallelize(words)\n",
    "type(words_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b2d1ded-db1e-4a9e-b6f1-659cdf5ffb5e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "count = words_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87b77e09-e1bc-415d-a047-f53e9375a56a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[9]: int"
     ]
    }
   ],
   "source": [
    "type(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1652d7d-25e8-4756-b4b0-83a9882b4b38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: ['scale',\n 'Java',\n 'Hadoop',\n 'spark',\n 'Akka',\n 'spark vs Hadoop',\n 'pyspark',\n 'pyspark and spark']"
     ]
    }
   ],
   "source": [
    "words_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce4aa10c-4115-40d2-8d10-a38a5791fafb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: list"
     ]
    }
   ],
   "source": [
    "type(words_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11d31f70-175b-4362-b13e-4e22e81dc154",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[19]: ['spark', 'spark vs Hadoop', 'pyspark', 'pyspark and spark']"
     ]
    }
   ],
   "source": [
    "word_filter = words.filter(lambda x: 'spark' in x)\n",
    "filtered = word_filter.collect()\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "321d5742-4073-4eea-9301-dba1ab6f02b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[20]: ['spark vs Hadoop', 'pyspark and spark']"
     ]
    }
   ],
   "source": [
    "word_filter = words.filter(lambda x: len(x) > 7)\n",
    "filtered = word_filter.collect()\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7fe2008-2f04-4095-8850-baa7202691dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[21]: [('scale', 1),\n ('Java', 1),\n ('Hadoop', 1),\n ('spark', 1),\n ('Akka', 1),\n ('spark vs Hadoop', 1),\n ('pyspark', 1),\n ('pyspark and spark', 1)]"
     ]
    }
   ],
   "source": [
    "word_map = words.map(lambda x: (x, 1))\n",
    "mapping = word_map.collect()\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7baa9a36-11a4-4dce-9477-c9abb99a575a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[22]: [('spark', (1, 2))]"
     ]
    }
   ],
   "source": [
    "rdd1 = sparkContext.parallelize([(\"spark\", 1), (\"hadoop\", 4)])\n",
    "rdd2 = sparkContext.parallelize([(\"spark\", 2), (\"Hadoop\", 5)])\n",
    "joined = rdd1.join(rdd2)\n",
    "result = joined.collect()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dff5e78e-6415-4cf6-9d0c-b99b9add2b87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SparkSession",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
